{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae9e3fc-cd0b-4f85-9627-fa7e523e9fa1",
   "metadata": {},
   "source": [
    "# 1.0 Environment:\n",
    "\n",
    "The model is trained with PyTorch and thus can be used in Python environments. The following packages meet the minimal requirements to load the model. \n",
    "\n",
    "*The Python version used in the pre-training phase is Python 3.11.5.\n",
    "\n",
    "|   Package   |   Version   |   Note                                    |\n",
    "|-------------|:-----------:|------------------------------------------:|\n",
    "|  timm       |  1.0.10     |Basic implementations of transformer models|\n",
    "|  torch      |2.4.1        |Deep learning framework                    |\n",
    "\n",
    "\n",
    "Codes written by Liuyin Yang (liuyin.yang@kuleuven.be)\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b329eb-8c0d-4b58-a89a-4ff1d61f4e95",
   "metadata": {},
   "source": [
    "# 1.1 Initialize a ViT model instance:\n",
    "\n",
    "An MAE model contains an encoder and a decoder, which are used in the pre-training phase to perform a reconstruction task. Both encoder and decoder are stored in the checkpoint file but for the downstreaming class, we will only use the encoder part.\n",
    "\n",
    "The ViT model shares the same backbone of the MAE encoder and decoder model. It is used for the downstreaming classification task. There are a few differences for example:\n",
    "\n",
    "1. num_classes defines number of classification classes (the number of last fc layer output neurons) \n",
    "2. drop_path_rate defines the droppath rate\n",
    "3. global_pool if True, using the global average pooling on all tokens as the final representation for the last layer input. If False, one can alternatively use the class token as the final representation. \n",
    "\n",
    "* Notice that in the MAE pretraining phase, there is no concept of num_classes. The num_classes are used to initialize a new fully connected layer that is concatenated at the end of the model. Typically, it needs to be trained to adapt to the downstream classification task.\n",
    "\n",
    "* Notice this model works with EEG data @128Hz, bandpassed between 0.5-64Hz, standardized per channel (z-standardization)\n",
    "\n",
    "All rights reserved.\n",
    "\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfeec4f-2c3a-4c74-bbb9-6ac1cdf7d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vsc-hard-mounts/leuven-data/343/vsc34340/miniconda3/envs/torch_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gloabl pool: True\n"
     ]
    }
   ],
   "source": [
    "import models_vit_eeg\n",
    "vit_model_variant = \"vit_small_patch16\"\n",
    "encoder_model = models_vit_eeg.__dict__[vit_model_variant](\n",
    "                        num_classes=4,\n",
    "                        drop_path_rate=0.1,\n",
    "                        global_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa997b77-cbf6-4a37-b551-d3d5b6df352a",
   "metadata": {},
   "source": [
    "# 1.2 Load the vit model weights from a checkpoint: \n",
    "To load the encoder, which is part of the mae model, you can run the following codes. It tries to find the matching weights from the checkpoint and disregard the others. In the end, you can also try to initialize the weights of the last fc layer for your training: using the trunc_normal_()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d0ba30-411e-45f6-975c-3896d2200366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from timm.models.layers import trunc_normal_\n",
    "check_point_dir = \"/lustre1/project/stg_00160/new_eeg_mae/small/checkpoint-200.pth\"\n",
    "global_pool = True\n",
    "checkpoint = torch.load(check_point_dir, map_location='cpu')\n",
    "checkpoint_model = checkpoint['model']\n",
    "state_dict = encoder_model.state_dict()\n",
    "for k in ['head.weight', 'head.bias']:\n",
    "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        #print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "\n",
    "# load pre-trained model\n",
    "msg = encoder_model.load_state_dict(checkpoint_model, strict=False)\n",
    "# Some sanity checks\n",
    "if global_pool:\n",
    "    assert set(msg.missing_keys) == {'pos_embed','head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'}\n",
    "else:\n",
    "    assert set(msg.missing_keys) == {'pos_embed','head.weight', 'head.bias'}\n",
    "\n",
    "# manually initialize fc layer\n",
    "#trunc_normal_(encoder_model.head.weight, std=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0031a-9c5e-44b9-bb80-fa92ab9c9dea",
   "metadata": {},
   "source": [
    "# 2.1 Passing data to the ViT model \n",
    "All Pytorch models work with torch.tensor, which can be converted from numpy.array. For the ViT model, it needs two inputs: eeg data in a shape of (batch, channel, time_points), sensloc in a shape of (batch, channel). The output is the last fc layer output that relates to the probability of your number of classes in a shape of (batch, num_classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3bfb44-7618-41b9-8de4-425c43c7dbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 384]) torch.Size([32])\n",
      "after unsqueezing: torch.Size([1, 32, 384]) torch.Size([1, 32])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# construct fake EEG input data from numpy\n",
    "import numpy as np\n",
    "    eeg_data = np.random.rand(32,128*3) # construct a 3-second 32-channel EEG data sampled at 128 Hz\n",
    "    eeg_senloc = np.random.randint(0,145,32) # construct its corresponding channel index \n",
    "\n",
    "    # convert the data to torch.tensor\n",
    "    eeg_data = torch.from_numpy(eeg_data).float()\n",
    "    eeg_senloc = torch.from_numpy(eeg_senloc).int()\n",
    "    print(eeg_data.shape, eeg_senloc.shape)\n",
    "\n",
    "    # create the batch dimension to be 1\n",
    "    eeg_data = torch.unsqueeze(eeg_data,0)\n",
    "    eeg_senloc = torch.unsqueeze(eeg_senloc,0)\n",
    "print(\"after unsqueezing:\", eeg_data.shape, eeg_senloc.shape)\n",
    "\n",
    "# pass into the vit model\n",
    "out = encoder_model(eeg_data, eeg_senloc)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0582d-3a44-46bb-b748-432b35ae004f",
   "metadata": {},
   "source": [
    "# 2.2 Channel index \n",
    "Each channel has a unique index for the channel positional embeddings. This can be retrived from the senloc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e61f373-f683-436b-b549-88504fab04cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  60 112 133  71  25  11 118  49  18  34  19  31  65 130 110   2  68\n",
      "  55  51  85  46  52  41  74   1 116  21  63  95   5 103  37  86  13  39\n",
      "  72  42  54   3  70  14  75  20   0  10 127  84 119  62  90  99 117  23\n",
      "  96 102  58 101 122  22 111  16 114 129  45  87  89  27  44  38  24  29\n",
      "  26 100  28  53 121 109  77   7 120 106  32 126  94  67  15 104 115  80\n",
      "   9 128  35 123  47  91  36  43  57 125 107 105 108  78  82  33  48  76\n",
      "   4 113  64  92  79 124 132  40  30   8  97  98  83  69  56  59 131  17\n",
      "  88  12]\n",
      "{'C1': 0, 'Pz': 1, 'C4': 2, 'F6': 3, 'FTT8h': 4, 'Oz': 5, 'Fp1': 6, 'FCC5h': 7, 'TPP8h': 8, 'CPP6h': 9, 'C2': 10, 'F4': 11, 'OI2h': 12, 'AF4': 13, 'FCz': 14, 'CCP6h': 15, 'TP8': 16, 'POO10h': 17, 'FC1': 18, 'FC6': 19, 'C5': 20, 'P8': 21, 'FT8': 22, 'P6': 23, 'P9': 24, 'Fz': 25, 'AFF1': 26, 'TPP10h': 27, 'AFF2': 28, 'P10': 29, 'CPP2h': 30, 'M1': 31, 'FCC6h': 32, 'FTT7h': 33, 'FC2': 34, 'PPO2': 35, 'AFp3h': 36, 'AF7': 37, 'PO10': 38, 'AF8': 39, 'CPP1h': 40, 'P7': 41, 'F1': 42, 'AFp4h': 43, 'PO9': 44, 'FT9': 45, 'CP2': 46, 'Iz': 47, 'FCC1h': 48, 'FC5': 49, 'T5': 50, 'CP5': 51, 'CP6': 52, 'FFC5h': 53, 'F2': 54, 'M2': 55, 'POO9h': 56, 'AFF5h': 57, 'PO4': 58, 'POO3h': 59, 'Fp2': 60, 'T3': 61, 'CP4': 62, 'POz': 63, 'TTP7h': 64, 'T7': 65, 'A2': 66, 'CCP4h': 67, 'T8': 68, 'PPO10h': 69, 'FC3': 70, 'F3': 71, 'F5': 72, 'A1': 73, 'P3': 74, 'FC4': 75, 'FCC2h': 76, 'FFC6h': 77, 'FFT8h': 78, 'CCP2h': 79, 'CPP4h': 80, 'T6': 81, 'FTT9h': 82, 'PPO6h': 83, 'CP3': 84, 'CP1': 85, 'AF3': 86, 'FT10': 87, 'OI1h': 88, 'TPP9h': 89, 'P5': 90, 'I2': 91, 'CCP1h': 92, 'T4': 93, 'CCP3h': 94, 'O1': 95, 'PO5': 96, 'PPO9h': 97, 'PPO5h': 98, 'P1': 99, 'AFz': 100, 'PO6': 101, 'PO3': 102, 'O2': 103, 'CPP5h': 104, 'FFC1h': 105, 'FCC4h': 106, 'FFT7h': 107, 'FFC2h': 108, 'FFC4h': 109, 'Cz': 110, 'TP7': 111, 'Fpz': 112, 'FTT10h': 113, 'PO7': 114, 'CPP3h': 115, 'P4': 116, 'P2': 117, 'F8': 118, 'CPz': 119, 'FCC3h': 120, 'FFC3h': 121, 'FT7': 122, 'I1': 123, 'TTP8h': 124, 'AFF6h': 125, 'CCP5h': 126, 'C6': 127, 'PPO1': 128, 'PO8': 129, 'C3': 130, 'POO4h': 131, 'TPP7h': 132, 'F7': 133, 'T9': 134, 'TP9': 135, 'T10': 136, 'TP10': 137, 'POO1': 138, 'POO2': 139, 'PPO1h': 140, 'PPO2h': 141}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/vsc-hard-mounts/leuven-data/343/vsc34340/new_eeg_mae/senloc_file/sen_chan_idx.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "# you can get the channel index for the high gamma dataset as\n",
    "hgd_chan_idx = data['hgd']\n",
    "# you can also check per channel index:\n",
    "chan_idx_map = data['channels_mapping']\n",
    "\n",
    "print(hgd_chan_idx)\n",
    "print(chan_idx_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b7221-d6d3-41d9-b770-b880e8faa419",
   "metadata": {},
   "source": [
    "# 3.1 Get attention scores in a forward pass\n",
    "By default timm's ViT implementations do not have the attention score outputs. Therefore, we need some triks in the model forward call, in order to save the model per-layer attention scores. This requires some manipulations of the timm ViT implementations, which can be done by replacing the original vision_transformer.py file by the one in the folder: /adaptation.  The following example shows a way to get the attention score after you replace the vision_transformer.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25d50d9-eed2-49af-a9a9-b72573d20dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 769, 769)\n"
     ]
    }
   ],
   "source": [
    "encoder_model.eval()\n",
    "avg_class0_attention_score = np.zeros((8,8,769,769)) # 8 layers, 8 heads, 769x769\n",
    "avg_class1_attention_score = np.zeros((8,8,769,769)) # 8 layers, 8 heads, 769x769\n",
    "\n",
    "# assume we have many trials that are of class 0 and 1\n",
    "# we wnat to save the final averaged attention to the avg_class0_attention_score array and the avg_class1_attention_score array\n",
    "# keeping all attention scores is memory-demanding, we sum them up and divide by the number of trials in th end\n",
    "\n",
    "# random 5 trials\n",
    "labels = [0,0,1,1,0] \n",
    "\n",
    "class_0_count = 0\n",
    "class_1_count = 0\n",
    "\n",
    "# first call this to enable tracking attention\n",
    "encoder_model.register_hook()\n",
    "\n",
    "for trial in range(5):\n",
    "    # random trial data\n",
    "    eeg_data = np.random.rand(32,128*3) # construct a 3-second 32-channel EEG data sampled at 128 Hz\n",
    "    eeg_senloc = np.random.randint(0,145,32) # construct its corresponding channel index \n",
    "\n",
    "    # convert the data to torch.tensor\n",
    "    eeg_data = torch.from_numpy(eeg_data).float()\n",
    "    eeg_senloc = torch.from_numpy(eeg_senloc).int()\n",
    "\n",
    "    # create the batch dimension to be 1\n",
    "    eeg_data = torch.unsqueeze(eeg_data,0)\n",
    "    eeg_senloc = torch.unsqueeze(eeg_senloc,0)\n",
    "\n",
    "    out = encoder_model(eeg_data, eeg_senloc)\n",
    "    \n",
    "    for layer in range(8):\n",
    "        if labels[trial] == 0:\n",
    "            class_0_count+=1\n",
    "            avg_class0_attention_score[layer,:,:,:] = avg_class0_attention_score[layer,:,:,:]+torch.squeeze(encoder_model.attn_scores[layer]).detach().cpu().numpy()\n",
    "        elif labels[trial] == 1:\n",
    "            class_1_count+=1\n",
    "            avg_class1_attention_score[layer,:,:,:] = avg_class1_attention_score[layer,:,:,:]+torch.squeeze(encoder_model.attn_scores[layer]).detach().cpu().numpy()\n",
    "        else:\n",
    "            print(\"wrong trial\", flush=True)\n",
    "    # remember to clear this internal list which stores the attention score for one forward call before calling the next (otherwise it will just concatenate new attention scores and you will soon run out of memory)\n",
    "    encoder_model.attn_scores=[]\n",
    "    \n",
    "    \n",
    "# calculate the avg attention\n",
    "avg_class0_attention_score = avg_class0_attention_score/class_0_count\n",
    "avg_class1_attention_score = avg_class1_attention_score/class_1_count\n",
    "    \n",
    "# print the attention score\n",
    "print(avg_class0_attention_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59a561-1c62-4fa1-a989-53b70f3ab63f",
   "metadata": {},
   "source": [
    "# 3.1.1 More details about the dimension of the attention score\n",
    "\n",
    "This attention is calculated on the basis of tokens. Therefore it's important to know how tokenization is done inside the model.\n",
    "Given an input EEG data in a shape of (num_channels, num_data_points), the model patches per channel and every #patch_size along the time dimension. The patch size for the vit_small_patch16 model is 16. In other words, every 16 data points (under a sampling rate of 128 Hz~=0.125s) will be converted into a token. So you can calculate the attention score matrix shape given an input data shape.\n",
    "\n",
    "Example: In the previous example, the input is (32,384), for each channel, 384 data points will be converted to 24 tokens, in the end we have 32x24+1 = 769 tokens (with an extra class token). After the self-attention (you can simply regard it as calculating pairwise similarity between all token pairs), you will have an attention map of 769x769 \n",
    "\n",
    "Another question you may ask is from the attention score matrix, how do I know each element belongs to which channel at which time. This is linked to the tokenization steps:\n",
    "\n",
    "1. Patchify\n",
    "\n",
    "EEG comes in as a tensor of shape (B,C,L) (batch, channels, time-samples). The model breaks each channel into non-overlapping patches of length ùëÉ (your patch_size), giving Seq = ùêø/ùëÉ patches per channel.\n",
    "\n",
    "2. Project & reshape\n",
    "\n",
    "After the linear projection you have shape (B,Seq,C,D). The model then flattens the middle two dims into a single ‚Äútoken‚Äù dimension (B,Seq√óC,D). Concretely, token index t‚àà[0,‚Ä¶,Seq√óC‚àí1] corresponds to time_patch=t/C,channel_idx=tmodC.\n",
    "\n",
    "3. Add a CLS token\n",
    "\n",
    "We concat one extra ‚ÄúCLS‚Äù token at index 0, so the total sequence length is N=1+Seq√óC.\n",
    "\n",
    "4. Attention scores\n",
    "\n",
    "After a forward pass with hooks, each block‚Äôs .attn_scores has shape (B,heads,N,N). Element attn[b,h,i,j] is ‚Äúhow much head h in example b attends from token i to token j.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51d231-3826-4a5d-9da7-adcb4142a6fd",
   "metadata": {},
   "source": [
    "# 3.1.2 Interpreting the per-layer attention score\n",
    "Gaining insights from computer vision, we can analyze the attention scores in the following way: attention rollout (check https://github.com/jacobgil/vit-explain). This is to mimic the attention flow inside the model.\n",
    "\n",
    "Using the attention scores per-layer we obtained previously, you can pass the following function to obtain the attention rollout that hopefully tells you where the model attend to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f83f26a3-183e-4739-a692-68ab5f566edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention rollout: torch.Size([769, 769])\n"
     ]
    }
   ],
   "source": [
    "# Attention rollout for estimating the flow of attention in ViT\n",
    "# Formula: Ar = (A(l)+I)*Ar(l-1), Ar(1) = A1+I\n",
    "def rollout(attentions, discard_ratio, head_fusion):\n",
    "    result = torch.eye(attentions.shape[-1])\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            #print(attention.shape)\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=0)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=0)[0]\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=0)[0]\n",
    "            elif head_fusion <12:\n",
    "                attention_heads_fused = attention[head_fusion]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(1, -1)\n",
    "\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 1e-10\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "            \n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    return result  \n",
    "\n",
    "\n",
    "roll_out_attention = rollout(attentions=torch.from_numpy(avg_class0_attention_score).type(torch.FloatTensor), discard_ratio=0.90, head_fusion=\"mean\")\n",
    "\n",
    "print(\"attention rollout:\", roll_out_attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15323c64-b878-42b6-bb52-8d1dd1a3d80f",
   "metadata": {},
   "source": [
    "# 3.1.3 Check the attention matrix structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47aa029e-b2e8-412a-88ea-e1a49b68955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128])\n",
      "tensor([[[1.0000, 1.0010, 1.0020, 1.0030, 1.0040, 1.0050, 1.0060, 1.0070,\n",
      "          1.0080, 1.0090, 1.0100, 1.0110, 1.0120, 1.0130, 1.0140, 1.0150,\n",
      "          1.0160, 1.0170, 1.0180, 1.0190, 1.0200, 1.0210, 1.0220, 1.0230,\n",
      "          1.0240, 1.0250, 1.0260, 1.0270, 1.0280, 1.0290, 1.0300, 1.0310,\n",
      "          1.0320, 1.0330, 1.0340, 1.0350, 1.0360, 1.0370, 1.0380, 1.0390,\n",
      "          1.0400, 1.0410, 1.0420, 1.0430, 1.0440, 1.0450, 1.0460, 1.0470,\n",
      "          1.0480, 1.0490, 1.0500, 1.0510, 1.0520, 1.0530, 1.0540, 1.0550,\n",
      "          1.0560, 1.0570, 1.0580, 1.0590, 1.0600, 1.0610, 1.0620, 1.0630,\n",
      "          1.0640, 1.0650, 1.0660, 1.0670, 1.0680, 1.0690, 1.0700, 1.0710,\n",
      "          1.0720, 1.0730, 1.0740, 1.0750, 1.0760, 1.0770, 1.0780, 1.0790,\n",
      "          1.0800, 1.0810, 1.0820, 1.0830, 1.0840, 1.0850, 1.0860, 1.0870,\n",
      "          1.0880, 1.0890, 1.0900, 1.0910, 1.0920, 1.0930, 1.0940, 1.0950,\n",
      "          1.0960, 1.0970, 1.0980, 1.0990, 1.1000, 1.1010, 1.1020, 1.1030,\n",
      "          1.1040, 1.1050, 1.1060, 1.1070, 1.1080, 1.1090, 1.1100, 1.1110,\n",
      "          1.1120, 1.1130, 1.1140, 1.1150, 1.1160, 1.1170, 1.1180, 1.1190,\n",
      "          1.1200, 1.1210, 1.1220, 1.1230, 1.1240, 1.1250, 1.1260, 1.1270],\n",
      "         [2.0000, 2.0010, 2.0020, 2.0030, 2.0040, 2.0050, 2.0060, 2.0070,\n",
      "          2.0080, 2.0090, 2.0100, 2.0110, 2.0120, 2.0130, 2.0140, 2.0150,\n",
      "          2.0160, 2.0170, 2.0180, 2.0190, 2.0200, 2.0210, 2.0220, 2.0230,\n",
      "          2.0240, 2.0250, 2.0260, 2.0270, 2.0280, 2.0290, 2.0300, 2.0310,\n",
      "          2.0320, 2.0330, 2.0340, 2.0350, 2.0360, 2.0370, 2.0380, 2.0390,\n",
      "          2.0400, 2.0410, 2.0420, 2.0430, 2.0440, 2.0450, 2.0460, 2.0470,\n",
      "          2.0480, 2.0490, 2.0500, 2.0510, 2.0520, 2.0530, 2.0540, 2.0550,\n",
      "          2.0560, 2.0570, 2.0580, 2.0590, 2.0600, 2.0610, 2.0620, 2.0630,\n",
      "          2.0640, 2.0650, 2.0660, 2.0670, 2.0680, 2.0690, 2.0700, 2.0710,\n",
      "          2.0720, 2.0730, 2.0740, 2.0750, 2.0760, 2.0770, 2.0780, 2.0790,\n",
      "          2.0800, 2.0810, 2.0820, 2.0830, 2.0840, 2.0850, 2.0860, 2.0870,\n",
      "          2.0880, 2.0890, 2.0900, 2.0910, 2.0920, 2.0930, 2.0940, 2.0950,\n",
      "          2.0960, 2.0970, 2.0980, 2.0990, 2.1000, 2.1010, 2.1020, 2.1030,\n",
      "          2.1040, 2.1050, 2.1060, 2.1070, 2.1080, 2.1090, 2.1100, 2.1110,\n",
      "          2.1120, 2.1130, 2.1140, 2.1150, 2.1160, 2.1170, 2.1180, 2.1190,\n",
      "          2.1200, 2.1210, 2.1220, 2.1230, 2.1240, 2.1250, 2.1260, 2.1270],\n",
      "         [3.0000, 3.0010, 3.0020, 3.0030, 3.0040, 3.0050, 3.0060, 3.0070,\n",
      "          3.0080, 3.0090, 3.0100, 3.0110, 3.0120, 3.0130, 3.0140, 3.0150,\n",
      "          3.0160, 3.0170, 3.0180, 3.0190, 3.0200, 3.0210, 3.0220, 3.0230,\n",
      "          3.0240, 3.0250, 3.0260, 3.0270, 3.0280, 3.0290, 3.0300, 3.0310,\n",
      "          3.0320, 3.0330, 3.0340, 3.0350, 3.0360, 3.0370, 3.0380, 3.0390,\n",
      "          3.0400, 3.0410, 3.0420, 3.0430, 3.0440, 3.0450, 3.0460, 3.0470,\n",
      "          3.0480, 3.0490, 3.0500, 3.0510, 3.0520, 3.0530, 3.0540, 3.0550,\n",
      "          3.0560, 3.0570, 3.0580, 3.0590, 3.0600, 3.0610, 3.0620, 3.0630,\n",
      "          3.0640, 3.0650, 3.0660, 3.0670, 3.0680, 3.0690, 3.0700, 3.0710,\n",
      "          3.0720, 3.0730, 3.0740, 3.0750, 3.0760, 3.0770, 3.0780, 3.0790,\n",
      "          3.0800, 3.0810, 3.0820, 3.0830, 3.0840, 3.0850, 3.0860, 3.0870,\n",
      "          3.0880, 3.0890, 3.0900, 3.0910, 3.0920, 3.0930, 3.0940, 3.0950,\n",
      "          3.0960, 3.0970, 3.0980, 3.0990, 3.1000, 3.1010, 3.1020, 3.1030,\n",
      "          3.1040, 3.1050, 3.1060, 3.1070, 3.1080, 3.1090, 3.1100, 3.1110,\n",
      "          3.1120, 3.1130, 3.1140, 3.1150, 3.1160, 3.1170, 3.1180, 3.1190,\n",
      "          3.1200, 3.1210, 3.1220, 3.1230, 3.1240, 3.1250, 3.1260, 3.1270]]])\n"
     ]
    }
   ],
   "source": [
    "#1 input data tokenization:\n",
    "import torch\n",
    "from models.models_vit_eeg import PatchEmbedEEG\n",
    "test_data = torch.randn(1,3,128) # 3 channel, 128 time points\n",
    "for channel in range(3):\n",
    "    for time in range(128):\n",
    "        test_data[0,channel,time] = 1+channel+time/1000\n",
    "        \n",
    "print(test_data.shape)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e3d09fc-3446-4ad9-8029-84817fbd8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patch = PatchEmbedEEG(patch_size=16, embed_dim=256)\n",
    "\n",
    "patched_out = test_patch.patchify_eeg(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6972e261-b8a1-48a4-83f5-e3103ad86391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 3, 16]) tensor([[[[1.0000, 1.0010, 1.0020, 1.0030, 1.0040, 1.0050, 1.0060, 1.0070,\n",
      "           1.0080, 1.0090, 1.0100, 1.0110, 1.0120, 1.0130, 1.0140, 1.0150],\n",
      "          [2.0000, 2.0010, 2.0020, 2.0030, 2.0040, 2.0050, 2.0060, 2.0070,\n",
      "           2.0080, 2.0090, 2.0100, 2.0110, 2.0120, 2.0130, 2.0140, 2.0150],\n",
      "          [3.0000, 3.0010, 3.0020, 3.0030, 3.0040, 3.0050, 3.0060, 3.0070,\n",
      "           3.0080, 3.0090, 3.0100, 3.0110, 3.0120, 3.0130, 3.0140, 3.0150]],\n",
      "\n",
      "         [[1.0160, 1.0170, 1.0180, 1.0190, 1.0200, 1.0210, 1.0220, 1.0230,\n",
      "           1.0240, 1.0250, 1.0260, 1.0270, 1.0280, 1.0290, 1.0300, 1.0310],\n",
      "          [2.0160, 2.0170, 2.0180, 2.0190, 2.0200, 2.0210, 2.0220, 2.0230,\n",
      "           2.0240, 2.0250, 2.0260, 2.0270, 2.0280, 2.0290, 2.0300, 2.0310],\n",
      "          [3.0160, 3.0170, 3.0180, 3.0190, 3.0200, 3.0210, 3.0220, 3.0230,\n",
      "           3.0240, 3.0250, 3.0260, 3.0270, 3.0280, 3.0290, 3.0300, 3.0310]],\n",
      "\n",
      "         [[1.0320, 1.0330, 1.0340, 1.0350, 1.0360, 1.0370, 1.0380, 1.0390,\n",
      "           1.0400, 1.0410, 1.0420, 1.0430, 1.0440, 1.0450, 1.0460, 1.0470],\n",
      "          [2.0320, 2.0330, 2.0340, 2.0350, 2.0360, 2.0370, 2.0380, 2.0390,\n",
      "           2.0400, 2.0410, 2.0420, 2.0430, 2.0440, 2.0450, 2.0460, 2.0470],\n",
      "          [3.0320, 3.0330, 3.0340, 3.0350, 3.0360, 3.0370, 3.0380, 3.0390,\n",
      "           3.0400, 3.0410, 3.0420, 3.0430, 3.0440, 3.0450, 3.0460, 3.0470]],\n",
      "\n",
      "         [[1.0480, 1.0490, 1.0500, 1.0510, 1.0520, 1.0530, 1.0540, 1.0550,\n",
      "           1.0560, 1.0570, 1.0580, 1.0590, 1.0600, 1.0610, 1.0620, 1.0630],\n",
      "          [2.0480, 2.0490, 2.0500, 2.0510, 2.0520, 2.0530, 2.0540, 2.0550,\n",
      "           2.0560, 2.0570, 2.0580, 2.0590, 2.0600, 2.0610, 2.0620, 2.0630],\n",
      "          [3.0480, 3.0490, 3.0500, 3.0510, 3.0520, 3.0530, 3.0540, 3.0550,\n",
      "           3.0560, 3.0570, 3.0580, 3.0590, 3.0600, 3.0610, 3.0620, 3.0630]],\n",
      "\n",
      "         [[1.0640, 1.0650, 1.0660, 1.0670, 1.0680, 1.0690, 1.0700, 1.0710,\n",
      "           1.0720, 1.0730, 1.0740, 1.0750, 1.0760, 1.0770, 1.0780, 1.0790],\n",
      "          [2.0640, 2.0650, 2.0660, 2.0670, 2.0680, 2.0690, 2.0700, 2.0710,\n",
      "           2.0720, 2.0730, 2.0740, 2.0750, 2.0760, 2.0770, 2.0780, 2.0790],\n",
      "          [3.0640, 3.0650, 3.0660, 3.0670, 3.0680, 3.0690, 3.0700, 3.0710,\n",
      "           3.0720, 3.0730, 3.0740, 3.0750, 3.0760, 3.0770, 3.0780, 3.0790]],\n",
      "\n",
      "         [[1.0800, 1.0810, 1.0820, 1.0830, 1.0840, 1.0850, 1.0860, 1.0870,\n",
      "           1.0880, 1.0890, 1.0900, 1.0910, 1.0920, 1.0930, 1.0940, 1.0950],\n",
      "          [2.0800, 2.0810, 2.0820, 2.0830, 2.0840, 2.0850, 2.0860, 2.0870,\n",
      "           2.0880, 2.0890, 2.0900, 2.0910, 2.0920, 2.0930, 2.0940, 2.0950],\n",
      "          [3.0800, 3.0810, 3.0820, 3.0830, 3.0840, 3.0850, 3.0860, 3.0870,\n",
      "           3.0880, 3.0890, 3.0900, 3.0910, 3.0920, 3.0930, 3.0940, 3.0950]],\n",
      "\n",
      "         [[1.0960, 1.0970, 1.0980, 1.0990, 1.1000, 1.1010, 1.1020, 1.1030,\n",
      "           1.1040, 1.1050, 1.1060, 1.1070, 1.1080, 1.1090, 1.1100, 1.1110],\n",
      "          [2.0960, 2.0970, 2.0980, 2.0990, 2.1000, 2.1010, 2.1020, 2.1030,\n",
      "           2.1040, 2.1050, 2.1060, 2.1070, 2.1080, 2.1090, 2.1100, 2.1110],\n",
      "          [3.0960, 3.0970, 3.0980, 3.0990, 3.1000, 3.1010, 3.1020, 3.1030,\n",
      "           3.1040, 3.1050, 3.1060, 3.1070, 3.1080, 3.1090, 3.1100, 3.1110]],\n",
      "\n",
      "         [[1.1120, 1.1130, 1.1140, 1.1150, 1.1160, 1.1170, 1.1180, 1.1190,\n",
      "           1.1200, 1.1210, 1.1220, 1.1230, 1.1240, 1.1250, 1.1260, 1.1270],\n",
      "          [2.1120, 2.1130, 2.1140, 2.1150, 2.1160, 2.1170, 2.1180, 2.1190,\n",
      "           2.1200, 2.1210, 2.1220, 2.1230, 2.1240, 2.1250, 2.1260, 2.1270],\n",
      "          [3.1120, 3.1130, 3.1140, 3.1150, 3.1160, 3.1170, 3.1180, 3.1190,\n",
      "           3.1200, 3.1210, 3.1220, 3.1230, 3.1240, 3.1250, 3.1260, 3.1270]]]])\n"
     ]
    }
   ],
   "source": [
    "print(patched_out.shape, patched_out) #Batch, Seq, Ch, L 128/16=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f879f6c-49df-4193-8383-90344c8f02aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0160, 1.0170, 1.0180, 1.0190, 1.0200, 1.0210, 1.0220, 1.0230, 1.0240,\n",
       "        1.0250, 1.0260, 1.0270, 1.0280, 1.0290, 1.0300, 1.0310])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_out[0,1,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c02a6024-c4bc-444d-834f-aadb683b80f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24, 16])\n"
     ]
    }
   ],
   "source": [
    "# then the model flatten this to a 1-d token vector:\n",
    "B, Seq, Ch_all, Dmodel = patched_out.shape\n",
    "Seq_total = Seq*Ch_all\n",
    "patched_out2 = patched_out.reshape(B,Seq_total,Dmodel)\n",
    "print(patched_out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e19f2650-23e2-45be-a3c8-5de7e279a4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0160, 1.0170, 1.0180, 1.0190, 1.0200, 1.0210, 1.0220, 1.0230, 1.0240,\n",
      "        1.0250, 1.0260, 1.0270, 1.0280, 1.0290, 1.0300, 1.0310])\n"
     ]
    }
   ],
   "source": [
    "print(patched_out2[0,3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33eea4-aaa0-4ff9-9210-9df5f1d26a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch_env",
   "language": "python",
   "name": "new_torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
